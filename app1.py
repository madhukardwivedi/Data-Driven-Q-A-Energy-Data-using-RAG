import pandas as pd
import psycopg2
from psycopg2 import sql
from sentence_transformers import SentenceTransformer
# from transformers import AutoTokenizer, AutoModelForCausalLM
import faiss
import numpy as np
import json
import datetime
import openai

open_ai_api = 'replace with your key'

# Load the dataset
file_path = './Sample_Energy_Data.csv'
data = pd.read_csv(file_path)

# Define PostgreSQL connection parameters
db_config = {
    'dbname': 'energy_data',  
    'user': 'myuser',       
    'password': 'mypassword',
    'host': 'localhost',
    'port': 5432
}

# Define the table creation query
def create_table():
    query = """
    CREATE TABLE IF NOT EXISTS energy_data (
        site_code VARCHAR(50),
        site_name VARCHAR(255),
        ca_number VARCHAR(50),
        board_name VARCHAR(255),
        closing_meter_reading FLOAT,
        opening_meter_reading FLOAT,
        unit_consumption FLOAT,
        bill_month VARCHAR(20),
        bill_date DATE,
        due_date DATE,
        amount_before_due_date FLOAT,
        amount_after_due_date FLOAT,
        sanctioned_load FLOAT,
        connected_load FLOAT,
        contract_demand FLOAT,
        kva_40_percent_demand FLOAT,
        tariff VARCHAR(50),
        category VARCHAR(50),
        early_payment_amount FLOAT,
        early_payment_date DATE,
        opening_meter_reading_date DATE,
        closing_meter_reading_date DATE,
        maximum_demand FLOAT,
        maximum_demand_kva FLOAT,
        excess_demand_charges FLOAT,
        power_factor FLOAT,
        power_factor_penalty FLOAT,
        green_tariff_kwh FLOAT,
        digital_payment_benefit FLOAT,
        late_payment_penalty FLOAT,
        PRIMARY KEY (site_code, bill_month)
    );
    """
    return query

# Function to insert data into the table
def insert_data(cursor, data):
    for _, row in data.iterrows():
        # Replace pandas NaT and NaN with None
        row = row.where(pd.notnull(row), None)
        
        insert_query = sql.SQL(
            """
            INSERT INTO energy_data (
                site_code, site_name, ca_number, board_name, closing_meter_reading,
                opening_meter_reading, unit_consumption, bill_month, bill_date, due_date,
                amount_before_due_date, amount_after_due_date, sanctioned_load, connected_load,
                contract_demand, kva_40_percent_demand, tariff, category, early_payment_amount,
                early_payment_date, opening_meter_reading_date, closing_meter_reading_date,
                maximum_demand, maximum_demand_kva, excess_demand_charges, power_factor,
                power_factor_penalty, green_tariff_kwh, digital_payment_benefit, late_payment_penalty
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (site_code, bill_month) DO NOTHING;
            """
        )
        cursor.execute(insert_query, tuple(row))
        

def detect_follow_up(query):
    """
    Detects if the query is a follow-up and retrieves relevant context from conversation history.

    Args:
        query (str): The current user query.

    Returns:
        dict: Relevant context from conversation history or None.
    """
    follow_up_keywords = ["it", "that", "those", "previous", "last", "sum"]
    if any(keyword in query.lower() for keyword in follow_up_keywords):
        return conversation_history[-1] if conversation_history else None
    return None

# # Global variable to store conversation history
conversation_history = []

def identify_columns_with_llm(query, column_names):
    """
    Identifies relevant columns and operations using LLM, with support for conversation history.

    Args:
        query (str): Current user query.
        column_names (list): List of column names in the schema.

    Returns:
        str: Instructions generated by the LLM.
    """
    try:
        openai.api_key = open_ai_api
        
        # Retrieve the latest context from conversation history
        context = conversation_history[-1] if conversation_history else None
        
        # Format schema for LLM
        schema_description = "\n".join([f"- {col}" for col in column_names])
        
        # Add context to the prompt if available
        context_description = ""
        if context:
            context_description = (
                f"Previous Query: {context['query']}\n"
                f"Previous Instructions: {context.get('instructions', 'None')}\n"
                f"Previous Results: {context.get('retrieved_results', 'None')}\n\n"
            )
        
        # Prepare the LLM prompt
        prompt = (
        "You are a highly intelligent and helpful assistant. Your task is to analyze a given database schema, "
        "consider the user's query, and determine the appropriate numeric operation to perform. You must also consider "
        "conversation history to provide contextually relevant responses.\n\n"
        "The operation should involve numeric calculations (e.g., sorting, averaging, summing).\n\n"
        "Schema:\n"
        f"{schema_description}\n\n"
        "Conversation History:\n"
        f"{context_description}\n"
        "Current Query:\n"
        f"{query}\n\n"
        "### Instructions ###\n"
        "1. If the query involves a numeric operation (e.g., sorting, averaging, summing), provide:\n"
        "   - Target column: The numeric column where the operation should be performed.\n"
        "   - Operation: The type of operation (e.g., sort descending, average).\n"
        "   - Related columns: Additional columns required to provide context or detailed output.\n\n"
        "2. If no numeric operation is required, return 'Target column: None'.\n\n"
        "### Few-Shot Examples ###\n\n"
        "#### Example 1 ####\n"
        "Query: What is the average power factor across all sites?\n"
        "Response:\n"
        "Target column: power_factor\n"
        "Operation: average\n"
        "Related columns: site_name\n\n"
        "#### Example 2 ####\n"
        "Query: Find the maximum unit consumption and the corresponding site.\n"
        "Response:\n"
        "Target column: unit_consumption\n"
        "Operation: max\n"
        "Related columns: site_name, site_code\n\n"
        "#### Example 3 ####\n"
        "Query: Show me the top 5 sites with the highest unit consumption.\n"
        "Response:\n"
        "Target column: unit_consumption\n"
        "Operation: sort descending\n"
        "Related columns: site_name, site_code\n\n"
        "#### Example 4 ####\n"
        "Query: What is the total sanctioned load across all sites?\n"
        "Response:\n"
        "Target column: sanctioned_load\n"
        "Operation: sum\n"
        "Related columns: None\n\n"
        "#### Example 5 ####\n"
        "Query: Find the minimum connected load among all sites.\n"
        "Response:\n"
        "Target column: connected_load\n"
        "Operation: min\n"
        "Related columns: None\n\n"
        "### Response Format ###\n"
        "Provide your response in this exact format:\n"
        "Target column: <target_column>\n"
        "Operation: <operation>\n"
        "Related columns: <related_column1>, <related_column2>, ..."
    )

        # Call the LLM API
        response = openai.ChatCompletion.create(
            model="gpt-4-turbo",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=300,
            temperature=0.5
        )
        
        # Extract and log column instructions
        column_instructions = response['choices'][0]['message']['content'].strip()
        print("Column Instructions from LLM:", column_instructions)
        
        # Add the current query and instructions to conversation history
        conversation_history.append({
            "query": query,
            "instructions": column_instructions,
            "retrieved_results": None  # Update this with actual results after retrieval
        })
        
        return column_instructions

    except Exception as e:
        print(f"Error identifying columns with LLM: {e}")
        return ""


def handle_numeric_operations(query, column_instructions, raw_data, column_names):
    """
    Handles numeric operations based on LLM instructions, logs results in conversation history.

    Args:
        column_instructions (str): Instructions from LLM about numeric operations.
        raw_data (list): Raw data rows from the database.
        column_names (list): Column names from the database schema.

    Returns:
        tuple: Processed data (top results or aggregation) and an error message if any.
    """
    try:
        # Extract target column, operation, and related columns
        target_column = None
        operation = None
        related_columns = []

        for line in column_instructions.split("\n"):
            if line.startswith("Target column:"):
                target_column = line.split(":", 1)[1].strip()
            elif line.startswith("Operation:"):
                operation = line.split(":", 1)[1].strip()
            elif line.startswith("Related columns:"):
                related_columns = [col.strip() for col in line.split(":", 1)[1].split(",")]

        if not target_column or target_column == "None":
            return None, "No numeric operation required."

        # Validate and find the index of the target column
        try:
            col_idx = column_names.index(target_column)
        except ValueError:
            raise ValueError(f"Target column '{target_column}' not found in column names.")

        # Filter rows with valid numeric data
        numeric_data = [
            row for row in raw_data if isinstance(row[col_idx], (int, float)) and row[col_idx] is not None
        ]

        if not numeric_data:
            raise ValueError(f"No valid numeric data found in the '{target_column}' column.")

        # Define supported operations
        operations = {
        "sort descending": lambda numeric_data: [
            {**{"value": row[col_idx]}, **{col: row[column_names.index(col)] for col in related_columns}}
            for row in sorted(numeric_data, key=lambda x: x[col_idx], reverse=True)[:5]
        ],
        "sort ascending": lambda numeric_data: [
            {**{"value": row[col_idx]}, **{col: row[column_names.index(col)] for col in related_columns}}
            for row in sorted(numeric_data, key=lambda x: x[col_idx])[:5]
        ],
        "average": lambda numeric_data: [
            {"average": sum(row[col_idx] for row in numeric_data) / len(numeric_data), "related_columns": related_columns}
        ],
        "sum": lambda numeric_data: [
            {"sum": sum(row[col_idx] for row in numeric_data), "related_columns": related_columns}
        ],
        "max": lambda numeric_data: [
            {**{"value": max(row[col_idx] for row in numeric_data)},
             **{col: row[column_names.index(col)] for col in related_columns
                for row in numeric_data if row[col_idx] == max(row[col_idx] for row in numeric_data)}}
        ],
        "min": lambda numeric_data: [
            {**{"value": min(row[col_idx] for row in numeric_data)},
             **{col: row[column_names.index(col)] for col in related_columns
                for row in numeric_data if row[col_idx] == min(row[col_idx] for row in numeric_data)}}
        ]
    }

        # Match operation and execute
        for op_keyword, operation_func in operations.items():
            if op_keyword in operation.lower():
                result = operation_func(numeric_data)

                # Log results in conversation history
                conversation_history.append({
                    "query": f"Operation: {operation} on {target_column}",
                    "instructions": column_instructions,
                    "results": result
                })

                return result

#         return None, f"Numeric operation '{operation}' not implemented."

    except Exception as e:
        print(f"Error in handle_numeric_operations: {e}")
        return None, f"Error in numeric operation: {e}"

# Function to retrieve and vectorize data
def vectorize_data():
    try:
        # Connect to PostgreSQL
        conn = psycopg2.connect(**db_config)
        cursor = conn.cursor()

        # Retrieve all columns from PostgreSQL
        cursor.execute("SELECT * FROM energy_data;")
        rows = cursor.fetchall()
        column_names = [desc[0] for desc in cursor.description]

        # Prepare data for vectorization
        text_data = []
        serializable_rows = []
        for row in rows:
            row_text = " ".join([f"{column_names[i]}: {str(value)}" for i, value in enumerate(row) if value is not None])
            text_data.append(row_text)

            # Convert date objects to strings for JSON serialization
            serializable_row = [value.isoformat() if isinstance(value, (datetime.date, datetime.datetime)) else value for value in row]
            serializable_rows.append(serializable_row)

        # Load sentence transformer model
        model = SentenceTransformer('all-MiniLM-L6-v2')

        # Generate embeddings
        embeddings = model.encode(text_data)

        # Initialize FAISS index
        dimension = embeddings.shape[1]
        index = faiss.IndexFlatL2(dimension)

        # Add embeddings to the index
        index.add(np.array(embeddings))

        # Save the index to a file
        faiss.write_index(index, 'faiss_index.bin')

        # Save the raw data to a JSON file
        with open('raw_data.json', 'w') as f:
            json.dump(serializable_rows, f)

        print("Data vectorized and stored in FAISS index.")
        return index, serializable_rows, column_names

    except Exception as e:
        print("Error:", e)
    finally:
        if conn:
            cursor.close()
            conn.close()

# Function to load the FAISS index and raw data
def load_index_and_data():
    try:
        # Load the FAISS index
        index = faiss.read_index('faiss_index.bin')

        # Load the raw data
        with open('raw_data.json', 'r') as f:
            raw_data = json.load(f)

        print("Index and raw data loaded successfully.")
        return index, raw_data

    except Exception as e:
        print("Error loading index and data:", e)
        return None, None

# Function to perform semantic search and log results in history
def semantic_search(query, index, raw_data, column_names, model, top_k=10):
    try:
        # Generate query embedding
        query_embedding = model.encode([query])

        # Perform FAISS search
        distances, indices = index.search(np.array(query_embedding), top_k)

        # Retrieve top results
        results = [raw_data[idx] for idx in indices[0]]

        # Add query and results to conversation history
        conversation_history.append({"query": query, "retrieved_results": results})

#         print("\nRetrieved rows:")
#         for row in results:
#             formatted_row = " | ".join([f"{col}: {val}" for col, val in zip(column_names, row)])
#             print(formatted_row)

        return results

    except Exception as e:
        print("Error during semantic search:", e)
        return []

# Function to format results into a prompt
def create_general_prompt(query, results):
    prompt = "You are a helpful assistant. Use the following data to answer the query.\n\n"
    prompt += f"Query: {query}\n\nData:\n"
    for result in results:
        prompt += f"- Data Row: {result}\n"
    prompt += "\nAnswer:"
    return prompt

# Function to generate LLM response using GPT-4 Turbo
def generate_llm_response_from_results(query, sorted_data, column_names):
    """
    Generates an LLM response based on sorted results.

    Args:
        query (str): The original user query.
        sorted_data (list): The sorted data (top results).
        column_names (list): List of column names in the database.

    Returns:
        str: A human-readable response generated by LLM.
    """
    try:
        openai.api_key = open_ai_api

        # Validate sorted_data
        if not sorted_data or len(sorted_data) == 0:
            return "No data available to generate a response."

        # Format the results into a readable string
        results_str = "\n".join(
            [
                f"{i + 1}. " + ", ".join([f"{col}: {row[idx]}" for idx, col in enumerate(column_names) if idx < len(row)])
                for i, row in enumerate(sorted_data)
            ]
        )

        # Prepare the prompt
        prompt = (
            "You are a helpful assistant. Based on the following query and the sorted results, provide a concise summary.\n\n"
            f"Query: {query}\n\n"
            f"Results:\n{results_str}\n\n"
            "Provide a response to answer the query based on the results."
        )

        # Call the LLM to generate a response
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=200,
            temperature=0.7
        )

        return response['choices'][0]['message']['content'].strip()

    except Exception as e:
        print(f"Error generating LLM response: {e}")
        return "Unable to generate a response."



def process_numeric_result(query, result):
    """
    Formats the result of a numeric operation for response generation.

    Args:
        result: The result of the numeric operation (list, dict, or other formats).

    Returns:
        str: A formatted string for the result.
    """
    if isinstance(result, list):
        # Handle list results (e.g., top 5 rows)
        return "\n".join([str(row) for row in result])
    elif isinstance(result, dict):
        # Handle dictionary results (e.g., averages, sums)
        return ", ".join([f"{key}: {value}" for key, value in result.items()])
    else:
        # Handle unexpected result formats
        return str(result)
    
    prompt = (
    "You are a helpful assistant. Based on the given query and results, extract and format the answer in a clear and concise manner.\n\n"
    "Query:\n"
    f"{query}\n\n"
    "Results (only relevant fields):\n"
    f"{results}\n\n"
    "### Instructions ###\n"
    "1. Always start with a single sentence summarizing the result and its relevance to the query.\n"
    "2. For queries asking for a single result (e.g., the highest value), provide the answer in one line:\n"
    "   - Example: 'The site with the highest unit consumption is Pune-Pride Parmar with 161,252 units.'\n"
    "3. Avoid returning raw data. Ensure the response is formatted and easy to understand."
)

    # Main handling
    if result:
        formatted_result = process_numeric_result(result)
        print("Formatted Result:", formatted_result)  # Debugging log

        # Pass formatted result to LLM
        llm_input = f"The result of your query is:\n{formatted_result}"
        try:
            response = openai.ChatCompletion.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": llm_input}
                ],
                max_tokens=150,
                temperature=0.5
            )
            print("LLM Response:", response['choices'][0]['message']['content'])
        except Exception as e:
            print("Error generating LLM response:", e)
    else:
        print("No valid result to process.")
        


################## Streamlit APP #####################
import streamlit as st
from sentence_transformers import SentenceTransformer
import psycopg2

def main():
    st.title("Energy Data Analysis")
    st.write("Welcome to the energy data analysis dashboard.")

    # Step 1: Convert date columns
    st.write("Converting date columns...")
    date_columns = ['Bill Date', 'Due Date', 'Early Payment Date', 'Opening Meter Reading Date', 'Closing Meter Reading Date']
    try:
        for col in date_columns:
            data[col] = pd.to_datetime(data[col], errors='coerce')
        st.success("Date columns converted successfully.")
    except Exception as e:
        st.error(f"Error converting date columns: {e}")
        return

    # Step 2: Connect to PostgreSQL
    st.write("Connecting to the database...")
    try:
        conn = psycopg2.connect(**db_config)
        conn.autocommit = True
        cursor = conn.cursor()
        st.success("Database connected successfully.")
    except Exception as e:
        st.error(f"Error connecting to the database: {e}")
        return

    # Step 3: Create the table
    st.write("Creating table...")
    try:
        cursor.execute(create_table())
        st.success("Table created successfully.")
    except Exception as e:
        st.error(f"Error creating the table: {e}")
        return

    # Step 4: Insert the data
    st.write("Inserting data...")
    try:
        insert_data(cursor, data)
        st.success("Data inserted successfully.")
    except Exception as e:
        st.error(f"Error inserting data: {e}")
        return

    # Step 5: Vectorize and store data
    st.write("Vectorizing and storing data...")
    try:
        vector_index, raw_data, column_names = vectorize_data()
        if vector_index and raw_data and column_names:
            st.success("Data vectorized and stored successfully.")
        else:
            st.error("Vectorization failed. Check logs for details.")
            return
    except Exception as e:
        st.error(f"Error vectorizing data: {e}")
        return

    # Step 6: Load index and raw data
    st.write("Loading FAISS index and raw data...")
    try:
        index, raw_data = load_index_and_data()
        if index and raw_data:
            st.success("FAISS index and raw data loaded successfully.")
        else:
            st.error("Error loading index or raw data.")
            return
    except Exception as e:
        st.error(f"Error loading index and raw data: {e}")
        return

    # Step 7: Load the model
    st.write("Loading sentence transformer model...")
    try:
        model = SentenceTransformer('all-MiniLM-L6-v2')
        st.success("Model loaded successfully.")
    except Exception as e:
        st.error(f"Error loading model: {e}")
        return

    # Step 8: Query input
    st.subheader("Query Input")
    query = st.text_input("Enter your query:")
    if not query.strip():
        st.warning("Please enter a query.")
        return

    # Step 9: Identify relevant columns and operations
    st.subheader("Identifying relevant columns and operations...")
    try:
        column_instructions = identify_columns_with_llm(query, column_names)
        st.write(f"Column Instructions: {column_instructions}")
    except Exception as e:
        st.error(f"Error identifying columns: {e}")
        return

    # Step 10: Perform operations based on the query
    if "Target column" in column_instructions and "Target column: None" not in column_instructions:
        st.subheader("Performing numeric operations...")
        try:
            sorted_data = handle_numeric_operations(query, column_instructions, raw_data, column_names)
            if sorted_data:
                st.write("Top results after numeric operation:")
#                 st.write(sorted_data)
                llm_response = process_numeric_result(query, sorted_data)
                st.write("LLM Response:", llm_response)
            else:
                st.warning("No valid results for numeric operations.")
        except Exception as e:
            st.error(f"Error performing numeric operations: {e}")
            return
    else:
        # Perform semantic search if numeric operations are not required
        st.subheader("Performing semantic search...")
        try:
            results = semantic_search(query, index, raw_data, column_names, model)
            st.write("Semantic Search Results:")
#             st.write(results)
            llm_response = generate_llm_response_from_results(query, results, column_names)
            st.write("LLM Response:", llm_response)
        except Exception as e:
            st.error(f"Error performing semantic search: {e}")
            return

    # Close database connection
    try:
        if conn:
            cursor.close()
            conn.close()
            st.success("Database connection closed.")
    except Exception as e:
        st.error(f"Error closing database connection: {e}")

if __name__ == "__main__":
    main()